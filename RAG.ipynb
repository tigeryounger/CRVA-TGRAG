{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 倒排索引"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load embedding model\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain.load import dumps, loads\n",
    "\n",
    "def reciprocal_rank_fusion(results: list[list], k=60):\n",
    "    fused_scores = {}\n",
    "    for docs in results:\n",
    "        # Assumes the docs are returned in sorted order of relevance\n",
    "        for rank, doc in enumerate(docs):\n",
    "            doc_str = dumps(doc)\n",
    "            if doc_str not in fused_scores:\n",
    "                fused_scores[doc_str] = 0\n",
    "            previous_score = fused_scores[doc_str]\n",
    "            fused_scores[doc_str] += 1 / (rank + k) \n",
    "            #倒排排名融合核心: 更新文档的融合得分。对于每个文档，按其排名来调整分数。越排前的文档（rank 较小）会得到越高的分数，评分根据排名的倒数来递减（即 1 / (rank + k)）。\n",
    "\n",
    "    reranked_results = [\n",
    "        (loads(doc), score)\n",
    "        for doc, score in sorted(fused_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    ]\n",
    "    # 重新排序并构建结果列表。首先，按文档的融合得分排序（从高到低），然后将每个文档从字符串形式反序列化回原始文档对象。\n",
    "    return reranked_results\n",
    "\n",
    "embeddings = OpenAIEmbeddings(\n",
    "\n",
    ")\n",
    "\n",
    "# load data\n",
    "from langchain.document_loaders import CSVLoader\n",
    "loader = CSVLoader(\"../data/cve.csv\")\n",
    "documents = loader.load()\n",
    "\n",
    "# split documents\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=0)\n",
    "documents = text_splitter.split_documents(documents)\n",
    "\n",
    "from langchain.vectorstores import FAISS\n",
    "vectorstore = FAISS.from_documents(documents, embeddings)\n",
    "\n",
    "# create retriever\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "# create llm\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "\n",
    ")\n",
    "\n",
    "# create chain 使用 Langsmith 客户端从模板中拉取查询生成的提示（prompt），然后通过语言模型生成相关的查询\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langsmith import Client\n",
    "client = Client()\n",
    "prompt = client.pull_prompt(\"langchain-ai/rag-fusion-query-generation\")\n",
    "\n",
    "# generate queries\n",
    "generate_queries = (\n",
    "    prompt | llm | StrOutputParser() | (lambda x: x.split(\"\\n\"))\n",
    ")\n",
    "\n",
    "# rerank results\n",
    "# 倒排排名融合 算法，它将多个搜索引擎或查询结果列表中的文档进行融合，计算每个文档的融合得分，最终返回按得分排序的文档。通过这种方式，可以将不同查询来源的搜索结果综合在一起，提高结果的相关性和多样性。倒排排名融合的优势在于它能够根据多个来源的结果进行加权融合，从而减少单一搜索引擎可能存在的偏差。\n",
    "\n",
    "\n",
    "# create chain 将查询生成链、检索器、倒排排名融合函数结合起来，形成一个完整的查询链。这个链首先生成查询，接着用检索器进行查询并返回结果，最后通过倒排排名融合算法重新排序这些结果。\n",
    "chain = generate_queries | retriever.map() | reciprocal_rank_fusion\n",
    "\n",
    "# check input schema\n",
    "chain.input_schema.schema()\n",
    "\n",
    "# rerank results\n",
    "chain.invoke(\"what is CVE-2013-3900\")\n",
    "\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "template = \"\"\"Answer the question based only on the following context.\n",
    "If you don't find the answer in the context, just say that you don't know.\n",
    "\n",
    "Context: {context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "rag_fusion_chain = (\n",
    "    {\n",
    "        \"context\": chain,\n",
    "        \"question\": RunnablePassthrough()\n",
    "    }\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "rag_fusion_chain.invoke(\"what is CVE-2013-3900\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load embedding model\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "embeddings = OpenAIEmbeddings(\n",
    "\n",
    "\n",
    "# load data\n",
    "from langchain.document_loaders import CSVLoader\n",
    "loader = CSVLoader(\"../data/cve.csv\")\n",
    "documents = loader.load()\n",
    "\n",
    "# split documents\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=0)\n",
    "documents = text_splitter.split_documents(documents)\n",
    "\n",
    "# create vectorstore\n",
    "from langchain.vectorstores import FAISS\n",
    "vectorstore = FAISS.from_documents(documents, embeddings)\n",
    "\n",
    "# create retriever\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "# create keyword retriever\n",
    "from langchain.retrievers import BM25Retriever\n",
    "keyword_retriever = BM25Retriever.from_documents(documents)\n",
    "keyword_retriever.k =  3\n",
    "\n",
    "# test keyword retriever\n",
    "keyword_retriever.get_relevant_documents(\"tell me about CVE-2013-3900\")\n",
    "\n",
    "# create ensemble retriever 检索组合\n",
    "from langchain.retrievers import EnsembleRetriever\n",
    "ensemble_retriever = EnsembleRetriever(retrievers=[retriever, keyword_retriever], weights=[0.5, 0.5]) #标准检索和BM25检索各占据0.5\n",
    "\n",
    "# test ensemble retriever\n",
    "ensemble_retriever.get_relevant_documents(\"tell me about CVE-2013-3900\")\n",
    "\n",
    "# create llm\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "\n",
    ")\n",
    "\n",
    "# create document chain\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "\n",
    "template = \"\"\"\"\n",
    "You are a helpful assistant that answers questions based on the following context.\n",
    "If you don't find the answer in the context, just say that you don't know.\n",
    "Context: {context}\n",
    "\n",
    "Question: {input}\n",
    "\n",
    "Answer:\n",
    "\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "# Setup RAG pipeline\n",
    "rag_chain = (\n",
    "    {\"context\": ensemble_retriever,  \"input\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# response\n",
    "response = rag_chain.invoke('what is CVE-2013-3900')\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langsmith import Client\n",
    "from langchain.retrievers import BM25Retriever, EnsembleRetriever\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# 1. 设置 Langsmith 客户端和查询生成\n",
    "client = Client()\n",
    "prompt = client.pull_prompt(\"langchain-ai/rag-fusion-query-generation\")\n",
    "\n",
    "# 创建查询生成链\n",
    "generate_queries = (\n",
    "    prompt | llm | StrOutputParser() | (lambda x: x.split(\"\\n\"))\n",
    ")\n",
    "\n",
    "# 2. 创建倒排排名融合（Reciprocal Rank Fusion）算法\n",
    "def reciprocal_rank_fusion(results: list[list], k=60):\n",
    "    fused_scores = {}\n",
    "    for docs in results:\n",
    "        # Assumes the docs are returned in sorted order of relevance\n",
    "        for rank, doc in enumerate(docs):\n",
    "            doc_str = dumps(doc)\n",
    "            if doc_str not in fused_scores:\n",
    "                fused_scores[doc_str] = 0\n",
    "            previous_score = fused_scores[doc_str]\n",
    "            fused_scores[doc_str] += 1 / (rank + k) \n",
    "            #倒排排名融合核心: 更新文档的融合得分。对于每个文档，按其排名来调整分数。越排前的文档（rank 较小）会得到越高的分数，评分根据排名的倒数来递减（即 1 / (rank + k)）。\n",
    "\n",
    "    reranked_results = [\n",
    "        (loads(doc), score)\n",
    "        for doc, score in sorted(fused_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    ]\n",
    "    # 重新排序并构建结果列表。首先，按文档的融合得分排序（从高到低），然后将每个文档从字符串形式反序列化回原始文档对象。\n",
    "    return reranked_results\n",
    "\n",
    "# 3. 创建检索器和组合检索器\n",
    "vectorstore = FAISS.from_documents(documents, embeddings)\n",
    "retriever = vectorstore.as_retriever()\n",
    "keyword_retriever = BM25Retriever.from_documents(documents)\n",
    "ensemble_retriever = EnsembleRetriever(retrievers=[retriever, keyword_retriever], weights=[0.5, 0.5])\n",
    "\n",
    "# 4. 创建查询链和重新排序\n",
    "chain = generate_queries | ensemble_retriever.map() | reciprocal_rank_fusion\n",
    "\n",
    "# 5. 创建一个基于上下文的生成模型\n",
    "template = \"\"\"Answer the question based only on the following context.\n",
    "If you don't find the answer in the context, just say that you don't know.\n",
    "\n",
    "Context: {context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "# 最终结合生成查询、检索和倒排排名融合的管道\n",
    "rag_fusion_chain = (\n",
    "    {\n",
    "        \"context\": chain,\n",
    "        \"question\": RunnablePassthrough()\n",
    "    }\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# 6. 测试查询管道\n",
    "response = rag_fusion_chain.invoke(\"what is CVE-2013-3900\")\n",
    "print(response)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
